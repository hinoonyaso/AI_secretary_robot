# Migration Recovery Guide - ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨ í•´ê²° ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2026-03-01
**ìƒí™©**: STT/TTS/LLM ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ ê²€ì¦ ì‹¤íŒ¨
**ë‹´ë‹¹**: 10ë…„ì°¨ ë¡œë´‡ SW ì‹œë‹ˆì–´ ê°œë°œíŒ€ì¥

---

## ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½

### âœ… STT (Moonshine) - **í†µê³¼**
- sherpa-onnx í†µí•© ì„±ê³µ
- ë”ë¯¸ ì…ë ¥ í…ŒìŠ¤íŠ¸ ì´ìŠˆëŠ” í•´ê²°ë¨
- ì‹¤ì œ WAV ì…ë ¥ í…ŒìŠ¤íŠ¸ **í†µê³¼**

### âš ï¸ TTS (Piper) - **ë¯¸í†µê³¼**
**ë¬¸ì œì **:
1. âŒ `espeak-ng` ë°”ì´ë„ˆë¦¬ ì—†ìŒ
2. âŒ `edge-tts` Python ëª¨ë“ˆ ì—†ìŒ
3. âš ï¸ `piper` ë°”ì´ë„ˆë¦¬ ì—†ìŒ (CLI í˜¸ì¶œ ë°©ì‹ ì‚¬ìš© ì¤‘)

### âš ï¸ LLM (llama.cpp) - **ë¯¸í†µê³¼**
**ë¬¸ì œì **:
1. âŒ `llama-server` ë°”ì´ë„ˆë¦¬ ì—†ìŒ
2. âŒ Qwen2.5-1.5B GGUF ëª¨ë¸ íŒŒì¼ ì—†ìŒ
3. âš ï¸ Ollama ë¯¸ì„¤ì¹˜ (fallbackë„ ë¯¸ë™ì‘)

---

## ğŸ”§ í•´ê²° ë°©ì•ˆ (ìš°ì„ ìˆœìœ„ ìˆœ)

---

## PART 1: TTS ë³µêµ¬ (ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥)

### 1.1 espeak-ng ì„¤ì¹˜ (í•„ìˆ˜ fallback ì—”ì§„)

```bash
# espeak-ng ì„¤ì¹˜
sudo apt update
sudo apt install -y espeak-ng espeak-ng-data

# ì„¤ì¹˜ í™•ì¸
espeak-ng --version
# ê¸°ëŒ€ ì¶œë ¥: eSpeak NG text-to-speech: 1.50 ...

# í•œêµ­ì–´ ìŒì„± í…ŒìŠ¤íŠ¸
espeak-ng -v ko "ì•ˆë…•í•˜ì„¸ìš”" --stdout | aplay

# í•œêµ­ì–´ voice íŒŒì¼ í™•ì¸
ls /usr/lib/aarch64-linux-gnu/espeak-ng-data/voices/ko
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 5ë¶„

---

### 1.2 edge-tts ì„¤ì¹˜ (ì„ íƒì  í´ë¼ìš°ë“œ TTS)

```bash
# edge-tts Python ëª¨ë“ˆ ì„¤ì¹˜
pip3 install edge-tts

# ì„¤ì¹˜ í™•ì¸
python3 -c "import edge_tts; print('edge-tts installed')"

# í…ŒìŠ¤íŠ¸ (ì¸í„°ë„· í•„ìš”)
edge-tts --voice ko-KR-SunHiNeural --text "ì•ˆë…•í•˜ì„¸ìš”" --write-media /tmp/test_edge.mp3
aplay /tmp/test_edge.mp3 2>/dev/null || mpg123 /tmp/test_edge.mp3
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3ë¶„

---

### 1.3 Piper ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜ (ë¡œì»¬ TTS)

#### Option A: ì‚¬ì „ ë¹Œë“œ ë°”ì´ë„ˆë¦¬ ì‚¬ìš© (ê¶Œì¥)

```bash
# Piper ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ (ARM64 / aarch64)
cd /tmp
wget https://github.com/rhasspy/piper/releases/download/2023.11.14-2/piper_arm64.tar.gz

# ì••ì¶• í•´ì œ
tar -xvzf piper_arm64.tar.gz

# ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜
sudo cp piper/piper /usr/local/bin/
sudo chmod +x /usr/local/bin/piper

# í™•ì¸
piper --version
# ê¸°ëŒ€ ì¶œë ¥: piper version 1.2.0

# í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©)
echo "ì•ˆë…•í•˜ì„¸ìš”" | piper \
  --model /home/sang/dev_ws/AI_secretary_robot/models/tts/neurlang_piper_onnx_kss_korean/piper-kss-korean.onnx \
  --config /home/sang/dev_ws/AI_secretary_robot/models/tts/neurlang_piper_onnx_kss_korean/piper-kss-korean.onnx.json \
  --output_file /tmp/test_piper.wav

aplay /tmp/test_piper.wav
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 10ë¶„

#### Option B: ì†ŒìŠ¤ ë¹Œë“œ (ê³ ê¸‰)

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
sudo apt install -y cmake g++ libfmt-dev libspdlog-dev libpopt-dev

# Piper ì†ŒìŠ¤ í´ë¡ 
cd /home/sang/dev_ws/AI_secretary_robot/external
git clone https://github.com/rhasspy/piper.git
cd piper

# onnxruntime ë‹¤ìš´ë¡œë“œ (ARM64)
mkdir -p lib
cd lib
wget https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-linux-aarch64-1.17.1.tgz
tar -xvzf onnxruntime-linux-aarch64-1.17.1.tgz
cd ..

# ë¹Œë“œ
mkdir build && cd build
cmake .. -DONNXRUNTIME_DIR=../lib/onnxruntime-linux-aarch64-1.17.1
make -j$(nproc)

# ì„¤ì¹˜
sudo cp piper /usr/local/bin/
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 30ë¶„

---

### 1.4 TTS ê²€ì¦

```bash
# ROS2 ë…¸ë“œ ì¬ì‹¤í–‰ (espeak-ng fallback í…ŒìŠ¤íŠ¸)
cd /home/sang/dev_ws/AI_secretary_robot
source install/setup.bash
ros2 launch tts_cpp tts.launch.py

# ë³„ë„ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
ros2 topic pub /tts/text std_msgs/String "data: 'ì•ˆë…•í•˜ì„¸ìš”'" --once

# ê¸°ëŒ€ ì¶œë ¥:
# - /tts/engine: "espeak-ng"
# - /tts/audio_path: "/home/ubuntu/rover_ws/src/tts_cpp/output/tts_xxxxx.wav"
# - ìŠ¤í”¼ì»¤ì—ì„œ ìŒì„± ì¬ìƒë¨

# Piper ì—”ì§„ í…ŒìŠ¤íŠ¸ (Piper ì„¤ì¹˜ í›„)
ros2 param set /tts_node tts_engine "piper"
ros2 topic pub /tts/text std_msgs/String "data: 'ë¡œë²„ì…ë‹ˆë‹¤'" --once

# ê¸°ëŒ€ ì¶œë ¥:
# - /tts/engine: "piper"
# - ë” ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ì¬ìƒ
```

---

## PART 2: LLM ë³µêµ¬ (ì¤‘ê¸° ì‘ì—…)

### 2.1 Ollama ì„¤ì¹˜ (ì¦‰ì‹œ fallback ë³µêµ¬)

```bash
# Ollama ì„¤ì¹˜ (ê³µì‹ ìŠ¤í¬ë¦½íŠ¸)
curl -fsSL https://ollama.com/install.sh | sh

# ì„¤ì¹˜ í™•ì¸
ollama --version
# ê¸°ëŒ€ ì¶œë ¥: ollama version is 0.x.x

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start ollama
sudo systemctl enable ollama

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Qwen2.5-1.5B)
ollama pull qwen2.5:1.5b

# ëª¨ë¸ í™•ì¸
ollama list
# ê¸°ëŒ€ ì¶œë ¥:
# NAME              ID          SIZE     MODIFIED
# qwen2.5:1.5b      abc123...   1.2GB    1 minute ago

# í…ŒìŠ¤íŠ¸
ollama run qwen2.5:1.5b "ì•ˆë…•í•˜ì„¸ìš”"
# ê¸°ëŒ€ ì¶œë ¥: "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?" (í•œêµ­ì–´ ì‘ë‹µ)
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 15ë¶„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨)

---

### 2.2 llama.cpp ë¹Œë“œ (Planning ë¬¸ì„œ ì¤€ìˆ˜)

```bash
# 1. ì†ŒìŠ¤ í´ë¡ 
cd /home/sang/dev_ws/AI_secretary_robot/external
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# 2. CUDA ì§€ì› ë¹Œë“œ (Jetson Orin Nano)
mkdir build && cd build
cmake .. \
  -DGGML_CUDA=ON \
  -DCMAKE_CUDA_ARCHITECTURES=87 \
  -DBUILD_SHARED_LIBS=ON

cmake --build . --config Release -j$(nproc)

# 3. ì„œë²„ ë°”ì´ë„ˆë¦¬ í™•ì¸
ls -lh bin/llama-server
# ê¸°ëŒ€ í¬ê¸°: ~50MB

# 4. ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (í¸ì˜)
sudo ln -sf $(pwd)/bin/llama-server /usr/local/bin/llama-server
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 45ë¶„ (ë¹Œë“œ ì‹œê°„)

---

### 2.3 Qwen2.5-1.5B GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# 1. ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /home/sang/dev_ws/AI_secretary_robot/models/llm
cd /home/sang/dev_ws/AI_secretary_robot/models/llm

# 2. GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Q4_K_M ì–‘ìí™”)
wget https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf

# 3. ëª¨ë¸ íŒŒì¼ í™•ì¸
ls -lh qwen2.5-1.5b-instruct-q4_k_m.gguf
# ê¸°ëŒ€ í¬ê¸°: ~900MB

# 4. llama-server ì‹¤í–‰ í…ŒìŠ¤íŠ¸
llama-server \
  --model qwen2.5-1.5b-instruct-q4_k_m.gguf \
  --port 8081 \
  -ngl 99 \
  -c 2048 \
  -t 4 &

# 5. í—¬ìŠ¤ì²´í¬
sleep 5
curl http://localhost:8081/health
# ê¸°ëŒ€ ì¶œë ¥: {"status":"ok"}

# 6. ì¶”ë¡  í…ŒìŠ¤íŠ¸
curl http://localhost:8081/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# ì„œë²„ ì¢…ë£Œ
pkill llama-server
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 20ë¶„ (ë‹¤ìš´ë¡œë“œ ì‹œê°„)

---

### 2.4 LLM ROS2 ë…¸ë“œ ì„¤ì • ì—…ë°ì´íŠ¸

```bash
# params.yaml ìˆ˜ì •
nano /home/sang/dev_ws/AI_secretary_robot/src/ai/llm_cpp/config/params.yaml
```

**ì¶”ê°€í•  íŒŒë¼ë¯¸í„°**:
```yaml
llm_node:
  ros__parameters:
    # ê¸°ì¡´ ì„¤ì • ìœ ì§€...

    # llama.cpp ì„œë²„ ì„¤ì • (ì‹ ê·œ)
    llama_server_binary: "/usr/local/bin/llama-server"
    llama_model_path: "/home/sang/dev_ws/AI_secretary_robot/models/llm/qwen2.5-1.5b-instruct-q4_k_m.gguf"
    llama_port: 8081
    llama_ngl: 99              # GPU ì „ì²´ ë ˆì´ì–´ ì˜¤í”„ë¡œë”©
    llama_ctx_size: 2048
    llama_threads: 4

    # Ollama fallback (ê¸°ì¡´)
    ollama_base_url: "http://localhost:11434"
    ollama_model: "qwen2.5:1.5b"
```

---

### 2.5 LLM ê²€ì¦

```bash
# 1. Ollama ëª¨ë“œ í…ŒìŠ¤íŠ¸ (ì¦‰ì‹œ ê°€ëŠ¥)
cd /home/sang/dev_ws/AI_secretary_robot
source install/setup.bash

# Ollama ì„œë²„ ì‹œì‘
ollama serve &

# ROS2 ë…¸ë“œ ì‹¤í–‰
ros2 launch llm_cpp llm.launch.py

# ë³„ë„ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
ros2 topic pub /intent_router/chat_text std_msgs/String \
  "data: 'ë¡œë´‡ íŒ” ë“¤ì–´'" --once

# ê¸°ëŒ€ ì¶œë ¥:
# - /llm/provider: "ollama"
# - /llm/response: "{\"intent\": \"manipulator_move\", ...}"

# 2. llama.cpp ëª¨ë“œ í…ŒìŠ¤íŠ¸ (llama-server ë¹Œë“œ í›„)
# params.yamlì—ì„œ llama_server_binary ê²½ë¡œ ì„¤ì • í›„ ì¬ì‹œì‘
ros2 launch llm_cpp llm.launch.py

# ê¸°ëŒ€ ì¶œë ¥ (ë¡œê·¸):
# [INFO] [llm_node]: llama.cpp server started at http://localhost:8081
# [INFO] [llm_node]: LLM engine ready (provider: llama.cpp)

# ì¶”ë¡  í…ŒìŠ¤íŠ¸
ros2 topic pub /intent_router/chat_text std_msgs/String \
  "data: 'ìê¸°ì†Œê°œ í•´ì¤˜'" --once

# ê¸°ëŒ€ ì¶œë ¥:
# - /llm/provider: "llama.cpp"
# - /llm/response: "ì €ëŠ” ë¡œë²„ì…ë‹ˆë‹¤..."
```

---

## ğŸ¯ ë³µêµ¬ ìš°ì„ ìˆœìœ„

### Phase 1: ì¦‰ì‹œ ì‹¤í–‰ (30ë¶„)
1. âœ… **espeak-ng ì„¤ì¹˜** â†’ TTS fallback ë³µêµ¬
2. âœ… **Ollama ì„¤ì¹˜** â†’ LLM fallback ë³µêµ¬
3. âœ… **ê²€ì¦ í…ŒìŠ¤íŠ¸** â†’ ìŒì„± íŒŒì´í”„ë¼ì¸ ë™ì‘ í™•ì¸

**ëª©í‘œ**: ìµœì†Œí•œì˜ ìŒì„± íŒŒì´í”„ë¼ì¸ ë™ì‘ (espeak + Ollama)

---

### Phase 2: ì¤‘ê¸° ê°œì„  (2ì‹œê°„)
1. âš ï¸ **Piper ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜** â†’ TTS ìŒì§ˆ ê°œì„ 
2. âš ï¸ **llama.cpp ë¹Œë“œ** â†’ LLM ë¡œì»¬ ì¶”ë¡  ê²½ë¡œ í™•ë³´
3. âš ï¸ **Qwen GGUF ë‹¤ìš´ë¡œë“œ** â†’ Planning ë¬¸ì„œ ì¤€ìˆ˜

**ëª©í‘œ**: Planning ë¬¸ì„œ ì•„í‚¤í…ì²˜ 50% ë‹¬ì„±

---

### Phase 3: ì¥ê¸° ì™„ì„± (1ì¼)
1. ğŸ”µ **sherpa-onnx TTS ë§ˆì´ê·¸ë ˆì´ì…˜** â†’ Piper CLI ì œê±°
2. ğŸ”µ **LlamaServerManager êµ¬í˜„** â†’ ì„œë²„ ìƒëª…ì£¼ê¸° ê´€ë¦¬
3. ğŸ”µ **ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸** â†’ 100% Planning ì¤€ìˆ˜

**ëª©í‘œ**: 3ê°œ Codex í”„ë¡¬í”„íŠ¸ ì™„ì „ êµ¬í˜„

---

## ğŸ“‹ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

### TTS ë³µêµ¬
- [ ] espeak-ng ì„¤ì¹˜ ì™„ë£Œ (`espeak-ng --version`)
- [ ] edge-tts ëª¨ë“ˆ ì„¤ì¹˜ ì™„ë£Œ (`python3 -c "import edge_tts"`)
- [ ] piper ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜ ì™„ë£Œ (`piper --version`)
- [ ] TTS ë…¸ë“œ ì‹¤í–‰ ì„±ê³µ (`ros2 launch tts_cpp tts.launch.py`)
- [ ] espeak-ng TTS í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Piper TTS í…ŒìŠ¤íŠ¸ í†µê³¼ (piper ì„¤ì¹˜ í›„)

### LLM ë³µêµ¬
- [ ] Ollama ì„¤ì¹˜ ì™„ë£Œ (`ollama --version`)
- [ ] Qwen2.5:1.5b ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (`ollama list`)
- [ ] llama.cpp ë¹Œë“œ ì™„ë£Œ (`llama-server --version`)
- [ ] Qwen GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (900MB)
- [ ] LLM ë…¸ë“œ ì‹¤í–‰ ì„±ê³µ (Ollama ëª¨ë“œ)
- [ ] llama.cpp ì„œë²„ ì‹œì‘ ì„±ê³µ (ë¡œì»¬ ëª¨ë“œ)
- [ ] LLM ì¶”ë¡  í…ŒìŠ¤íŠ¸ í†µê³¼

### ì „ì²´ íŒŒì´í”„ë¼ì¸
- [ ] Wake Word ê²€ì¶œ â†’ STT â†’ LLM â†’ TTS í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì‹¤ì‹œê°„ ìŒì„± ëª…ë ¹ ì²˜ë¦¬ í™•ì¸
- [ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (< 3GB)
- [ ] Planning ë¬¸ì„œì™€ ì¼ì¹˜ë„ 80% ì´ìƒ

---

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### espeak-ng í•œêµ­ì–´ ìŒì„± í’ˆì§ˆ ë¶ˆëŸ‰
**ì¦ìƒ**: í•œêµ­ì–´ ë°œìŒì´ ë¶€ìì—°ìŠ¤ëŸ¬ì›€
**í•´ê²°**: Piper TTS ìš°ì„  ì‚¬ìš©, espeak-ngëŠ” ìµœì¢… fallbackë§Œ

### Piper ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
**ì¦ìƒ**: `Error loading model` ë©”ì‹œì§€
**í•´ê²°**:
```bash
# ëª¨ë¸ íŒŒì¼ ê¶Œí•œ í™•ì¸
chmod 644 /home/sang/dev_ws/AI_secretary_robot/models/tts/neurlang_piper_onnx_kss_korean/*.onnx

# espeak-ng-data ê²½ë¡œ í™•ì¸
ls /usr/lib/aarch64-linux-gnu/espeak-ng-data/
```

### llama-server OOM (ë©”ëª¨ë¦¬ ë¶€ì¡±)
**ì¦ìƒ**: ì„œë²„ ì‹œì‘ ì§í›„ ì¢…ë£Œë¨
**í•´ê²°**:
```bash
# 1. ZRAM í™•ì¸
swapon -s

# 2. ngl ê°’ ê°ì†Œ (GPU ë ˆì´ì–´ ì¤„ì´ê¸°)
llama-server --model model.gguf -ngl 50  # 99 â†’ 50

# 3. Context ìœˆë„ìš° ê°ì†Œ
llama-server --model model.gguf -c 1024  # 2048 â†’ 1024
```

### Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼
**ì¦ìƒ**: `ollama pull` ì§„í–‰ ì—†ìŒ
**í•´ê²°**:
```bash
# 1. í”„ë¡ì‹œ í™•ì¸
curl -I https://ollama.com

# 2. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
cd ~/.ollama/models
wget https://huggingface.co/...  # Hugging Faceì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ

# 3. Ollama ì¬ì‹œì‘
sudo systemctl restart ollama
```

---

## ğŸ“ ê¸´ê¸‰ ì§€ì›

**ìƒí™©ë³„ ëŒ€ì‘**:
1. **TTS ì™„ì „ ì‹¤íŒ¨** â†’ espeak-ngë§Œ ì‚¬ìš©, ìŒì§ˆ íƒ€í˜‘
2. **LLM ë¡œì»¬ ì‹¤íŒ¨** â†’ Ollama fallback ì˜ì¡´, ì¸í„°ë„· í•„ìš” ì‹œ í´ë¼ìš°ë“œ API
3. **ì „ì²´ íŒŒì´í”„ë¼ì¸ í¬ë˜ì‹œ** â†’ ê°œë³„ ë…¸ë“œ ìˆœì°¨ ì‹¤í–‰, ë¡œê·¸ ë¶„ì„

**ë¡œê·¸ ìˆ˜ì§‘**:
```bash
# ROS2 ë¡œê·¸ í™•ì¸
cat ~/.ros/log/latest/tts_node/stdout.log
cat ~/.ros/log/latest/llm_node/stdout.log

# llama-server ë¡œê·¸
cat /tmp/llama_server_8081.log

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
tegrastats | head -20
```

---

**ì‘ì„±ì ë…¸íŠ¸**:
ë³¸ ê°€ì´ë“œëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨ í›„ **ì¦‰ì‹œ ë³µêµ¬ ê°€ëŠ¥í•œ ê²½ë¡œ**ë¥¼ ìš°ì„  ì œì‹œí•©ë‹ˆë‹¤. Phase 1ë§Œ ì™„ë£Œí•´ë„ ê¸°ë³¸ ìŒì„± íŒŒì´í”„ë¼ì¸ì€ ë™ì‘í•˜ë©°, Phase 2/3ëŠ” Planning ë¬¸ì„œ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ì ì§„ì  ê°œì„ ì…ë‹ˆë‹¤. **espeak-ng + Ollama ì¡°í•©ì€ ì•ˆì •ì„± ìµœìš°ì„  fallback**ìœ¼ë¡œ, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
