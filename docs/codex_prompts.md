# JetRover "Rover" â€” ê²½ëŸ‰ Docker ì»¨í…Œì´ë„ˆ êµ¬ì¶• Codex í”„ë¡¬í”„íŠ¸ ëª¨ìŒ

> **ì‹¤í–‰ ìˆœì„œ**: Prompt 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 (ìˆœì„œëŒ€ë¡œ ì§„í–‰)
> **ëª©í‘œ**: 15GB `l4t-pytorch` ì´ë¯¸ì§€ ì œê±°, 1~2GB ê²½ëŸ‰ Ubuntu+ROS2 ì´ë¯¸ì§€ë¡œ ì „í™˜
> **í™˜ê²½**: Jetson Orin Nano 8GB / JetPack 6.x / CUDA 12.2 / ARM64 / Ubuntu 22.04

---

## ğŸ“¦ Prompt 1: Dockerfile â€” ê¸°ë°˜ ë ˆì´ì–´ (Base + System Deps + ROS 2 Humble)

```
[Context]
- Target hardware: NVIDIA Jetson Orin Nano 8GB (ARM64/aarch64)
- JetPack version: 6.x (Ubuntu 22.04, CUDA 12.2, cuDNN 8.9, TensorRT 8.6)
- Goal: Replace the heavy `nvcr.io/nvidia/l4t-pytorch:r36.3.0-pth2.4-py3` image (15GB)
         with a minimal base that has zero PyTorch dependency.
- Output file: `docker/Dockerfile` (multi-stage build, stage name: `base`)

[Critical Design Decision]
ROS 2 Humbleì€ ë°˜ë“œì‹œ ì´ ì²« ë²ˆì§¸ ìŠ¤í…Œì´ì§€(base)ì— ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ìœ :
  1. ì´í›„ ìŠ¤í…Œì´ì§€(Prompt 2~4)ì—ì„œ ë¹Œë“œí•˜ëŠ” C++ íŒ¨í‚¤ì§€ë“¤ì´ rclcpp, std_msgs ë“±
     ROS 2 í—¤ë”/ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì˜ì¡´í•©ë‹ˆë‹¤. CMakeLists.txtì˜ find_package(rclcpp)ê°€
     base ì´ë¯¸ì§€ì— ROS 2ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨í•©ë‹ˆë‹¤.
  2. ì»¨í…Œì´ë„ˆ ì•ˆì˜ ëª¨ë“  AI ë…¸ë“œ(STT, LLM, TTS, Vision)ëŠ” ROS 2 í† í”½ìœ¼ë¡œ ì„œë¡œ
     í†µì‹ í•©ë‹ˆë‹¤. ROS 2 DDS(FastDDS)ê°€ baseë¶€í„° ì„¤ì¹˜ë˜ì–´ì•¼ ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì´ í†µì¼ë©ë‹ˆë‹¤.
  3. host ROS 2 Humbleê³¼ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ê°€ ë™ì¼í•œ ROS_DOMAIN_ID(42)ë¡œ í†µì‹ í•˜ë ¤ë©´
     ë™ì¼í•œ DDS êµ¬í˜„ì²´(rmw_fastrtps_cpp)ê°€ baseì—ì„œë¶€í„° í™˜ê²½ë³€ìˆ˜ë¡œ ê³ ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

[Task]
Write the FIRST STAGE of a multi-stage Dockerfile for Jetson Orin Nano.

Requirements:
1. FROM: Use `nvcr.io/nvidia/l4t-base:r36.4.3` as the base image
   (NGC ARM64 base image for JetPack 6.0+, ~0.8GB, no PyTorch, no CUDA devel)
   âš ï¸ íƒœê·¸ ì•ˆì •ì„± ì£¼ì˜: r36.4.3 íƒœê·¸ê°€ NGCì—ì„œ ê°€ë” ì‚­ì œë˜ëŠ” ì‚¬ë¡€ê°€ ìˆìŠµë‹ˆë‹¤.
   Dockerfile ìµœìƒë‹¨ì— ARGë¡œ fallbackì„ ë°˜ë“œì‹œ êµ¬í˜„í•˜ì„¸ìš”:
     ARG L4T_TAG=r36.4.3
     FROM nvcr.io/nvidia/l4t-base:${L4T_TAG}
   ë¹Œë“œ ì‹œ fallback: docker build --build-arg L4T_TAG=r36.3.0 ...
   Alternative (includes CUDA devel tools): `nvcr.io/nvidia/l4t-cuda:12.2.12-devel`
   Best alternative (ROS 2 already included): `dustynv/ros:humble-ros-base-l4t-r36.3.0`
   NOTE: If using dustynv base, skip Block B entirely.

2. System packages to install (apt-get, no recommended, clean cache):
   - Build tools: cmake, ninja-build, build-essential, git, wget, curl, pkg-config
   - Audio: libasound2-dev, libpulse-dev, alsa-utils, portaudio19-dev
   - Networking: libcurl4-openssl-dev, libssl-dev
   - Utilities: python3-pip, python3-dev, sqlite3, libsqlite3-dev
   - Media: ffmpeg, libavcodec-dev, libavformat-dev, libswresample-dev
   - USB/devices: libusb-1.0-0-dev, udev
   - OpenCV (no contrib): libopencv-dev
   - ROS 2 colcon build tools: python3-colcon-common-extensions, python3-rosdep

3. [Block B] ROS 2 Humble ì„¤ì¹˜ (apt, ARM64):
   a. Add ROS 2 APT repository:
      curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key \
        | gpg --dearmor -o /usr/share/keyrings/ros-archive-keyring.gpg
      echo "deb [arch=arm64 signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] \
        http://packages.ros.org/ros2/ubuntu jammy main" \
        > /etc/apt/sources.list.d/ros2.list

   b. Install ROS 2 Humble packages (ros-base, NOT desktop):
      - ros-humble-ros-base           # rclcpp, std_msgs, geometry_msgs ë“± í•µì‹¬
      - ros-humble-rmw-fastrtps-cpp   # DDS êµ¬í˜„ì²´ (FastDDS)
      - ros-humble-ament-cmake        # colcon ë¹Œë“œ ì‹œìŠ¤í…œ
      - ros-humble-ament-index-cpp    # íŒ¨í‚¤ì§€ ì¸ë±ìŠ¤ C++ API
      - ros-humble-tf2-ros            # ì¢Œí‘œê³„ ë³€í™˜ (MoveIt, Nav2 í•„ìˆ˜)
      - ros-humble-sensor-msgs        # Image, PointCloud2, Imu ë©”ì‹œì§€
      - ros-humble-geometry-msgs      # Pose, Twist, Point ë©”ì‹œì§€
      - ros-humble-nav-msgs           # OccupancyGrid (SLAM ì§€ë„)
      - ros-humble-action-msgs        # ì•¡ì…˜ ì¸í„°í˜ì´ìŠ¤
      - ros-humble-lifecycle-msgs     # ë…¸ë“œ ìˆ˜ëª…ì£¼ê¸° ê´€ë¦¬
      NOTE: ros-humble-moveit, ros-humble-nav2-bringupëŠ” Prompt 4ì—ì„œ ì¶”ê°€

   c. Initialize rosdep:
      rosdep init && rosdep update --rosdistro humble

   d. Source ROS 2 in /etc/bash.bashrc:
      echo "source /opt/ros/humble/setup.bash" >> /etc/bash.bashrc

4. Environment variables to set:
   - CUDA_HOME=/usr/local/cuda
   - PATH=$PATH:$CUDA_HOME/bin
   - LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64
   - DEBIAN_FRONTEND=noninteractive
   - LANG=ko_KR.UTF-8
   - ROS_DISTRO=humble
   - AMENT_PREFIX_PATH=/opt/ros/humble
   - ROS_PYTHON_VERSION=3
   - RMW_IMPLEMENTATION=rmw_fastrtps_cpp      # hostì™€ ë™ì¼í•œ DDS êµ¬í˜„ì²´
   - ROS_DOMAIN_ID=42                          # host ROS 2ì™€ ë™ì¼í•œ ë„ë©”ì¸ ID
   - FASTRTPS_DEFAULT_PROFILES_FILE=/opt/rover/config/fastdds.xml

5. Install Korean locale support (locales package, generate ko_KR.UTF-8)

6. Create directory structure inside the image:
   /opt/rover/models/stt/       # Moonshine ONNX models
   /opt/rover/models/embedding/ # KoSimCSE ONNX models
   /opt/rover/models/llm/       # GGUF models (bind-mounted from SSD at runtime)
   /opt/rover/models/tts/       # Piper TTS ONNX voices
   /opt/rover/models/vision/    # YOLO11n TensorRT engine
   /opt/rover/db/               # SQLite database
   /opt/rover/ws/               # ROS 2 workspace (colcon install ê²°ê³¼ë¬¼ mount)
   /opt/rover/scripts/          # Runtime scripts
   /opt/rover/config/           # FastDDS XML, ALSA ì„¤ì • ë“±

7. [Block C] FastDDS XML ì„¤ì • íŒŒì¼ ì‘ì„± (/opt/rover/config/fastdds.xml):
   ëª©ì : ì»¨í…Œì´ë„ˆ ë‚´ë¶€ â†” í˜¸ìŠ¤íŠ¸ ROS 2 Humble ê°„ DDS í†µì‹  í™œì„±í™”
   ì„¤ì • ë‚´ìš©:
   - transport: UDPv4 (network_mode: host ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ host ë„¤íŠ¸ì›Œí¬ ê³µìœ )
   - participant name: "rover_container"
   - builtin discovery: SIMPLE (ê¸°ë³¸ê°’ ìœ ì§€)
   - history depth: 10 (ì¼ë°˜ í† í”½), 1 (ì„¼ì„œ í† í”½)
   ì˜ˆì‹œ êµ¬ì¡°:
     <profiles>
       <participant profile_name="default_profile" is_default_profile="true">
         <rtps>
           <name>rover_container</name>
           <builtin><discovery_config>...</discovery_config></builtin>
         </rtps>
       </participant>
     </profiles>

8. Add a build ARG: `JETPACK_VERSION=36.4.3`

[Constraints]
- NO pip install torch, torchvision, torchaudio â€” zero PyTorch allowed
- NO conda
- Use --no-install-recommends for all apt-get
- RUN layers must be merged where possible to minimize layer count
- Add inline comments explaining each major block
- ROS 2ëŠ” ros-humble-ros-baseë§Œ ì„¤ì¹˜ (desktop, rviz, rqt ê¸ˆì§€ â€” ìš©ëŸ‰ ì ˆê°)
- Final image layer must not exceed 600MB beyond the base image (ROS 2 í¬í•¨)

[Verification step â€” RUN layerë¡œ í¬í•¨í•  ê²ƒ]
RUN bash -c "source /opt/ros/humble/setup.bash && \
             ros2 --version && \
             python3 -c 'import rclpy; print(\"rclpy OK\")'"

[Output format]
- Single Dockerfile content for stage `base`
- Include a comment block at the top with image size estimate
- Include `LABEL` metadata: version, maintainer, description
- Include /opt/rover/config/fastdds.xml content in a heredoc within the Dockerfile
```

---

## ğŸ§  Prompt 2: Dockerfile â€” AI ì—”ì§„ ë ˆì´ì–´ (ONNX Runtime + llama.cpp)

```
[Context â€” ì´ì „ Prompt 1ì˜ ê²°ê³¼ë¬¼ ìœ„ì— ì´ì–´ì„œ ì‘ì„±]
- Previous stage: `base` (l4t-base:r36.4.3 + system deps + ROS 2 Humble)
- This stage adds the AI inference runtimes (NO Python ML frameworks, NO PyTorch)
- Output: adds stage `ai-runtime` to `docker/Dockerfile`

[Task]
Write the SECOND STAGE (`FROM base AS ai-runtime`) of the multi-stage Dockerfile.

Block A â€” ONNX Runtime GPU (ARM64 / JetPack 6):
  âš ï¸ IMPORTANT: ORT 1.17+ ì†ŒìŠ¤ ë¹Œë“œëŠ” CMake 3.26+ê°€ í•„ìš”í•©ë‹ˆë‹¤.
     jammy ê¸°ë³¸ CMake(3.22.1)ì´ë¯€ë¡œ Kitware APTë¡œ ë¨¼ì € ì—…ê·¸ë ˆì´ë“œí•´ì•¼ í•©ë‹ˆë‹¤.

  1. TensorRT ëŸ°íƒ€ì„/ê°œë°œ íŒ¨í‚¤ì§€ ì‚¬ì „ ì„¤ì¹˜:
     apt-get install -y --no-install-recommends \
       libnvinfer8 libnvinfer-dev libnvinfer-plugin8 libnvinfer-plugin-dev \
       libnvparsers8 libnvparsers-dev libnvonnxparsers8 libnvonnxparsers-dev

  2. CMake ì—…ê·¸ë ˆì´ë“œ (Kitware APT):
     wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null \
       | gpg --dearmor > /usr/share/keyrings/kitware-archive-keyring.gpg
     echo 'deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ jammy main' \
       > /etc/apt/sources.list.d/kitware.list
     apt-get update && apt-get install -y --no-install-recommends cmake
     cmake --version   # 3.26+ í™•ì¸

  3. dusty-nv/jetson-containersë¡œ ORT wheel ìë™ ë¹Œë“œ:
     git clone https://github.com/dusty-nv/jetson-containers /tmp/jc
     cd /tmp/jc/packages/ml/onnxruntime
     export UV_SYSTEM_PYTHON=1 ONNXRUNTIME_VERSION=1.17.0 ONNXRUNTIME_BRANCH=v1.17.0 ONNXRUNTIME_FLAGS=--allow_running_as_root CUDA_ARCHITECTURES=87
     ./build.sh --use-cache --no-deps

  4. âš ï¸ --no-deps ì´í›„ í•„ìˆ˜ ì˜ì¡´ì„± ìˆ˜ë™ ì„¤ì¹˜ (ì—†ìœ¼ë©´ import ì‹œ ModuleNotFoundError):
     pip3 install \
       numpy==1.24.4 \        # ort numpy backend (1.25+ ì™€ í˜¸í™˜ì„± ì´ìŠˆ ìˆìŒ)
       onnx==1.15.0 \         # onnx protobuf model ë¡œë”©ìš©
       protobuf==3.20.3 \     # onnx ì˜ì¡´ì„± (4.xì™€ ì¶©ëŒ ì£¼ì˜)
       coloredlogs \          # ort ëŸ°íƒ€ì„ ë¡œê¹…
       flatbuffers            # ort ë‚´ë¶€ ì§ë ¬í™”
     # torch, torchvisionì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ

  5. Verify:
     python3 -c "import onnxruntime as ort; print('SUCCESS:', ort.__version__, ort.get_device())"
     â†’ ë°˜ë“œì‹œ 'GPU' ì¶œë ¥ í™•ì¸ (CPUë©´ wheelì´ ì˜ëª»ëœ ê²ƒ)

  6. Cleanup: rm -rf /tmp/jc /root/.cache/pip

  7. Environment variables:
     ORT_TENSORRT_ENGINE_CACHE_ENABLE=1
     ORT_TENSORRT_ENGINE_CACHE_PATH=/opt/rover/models/.trt_cache

Block B â€” llama.cpp (CUDA backend, GGUF support):
  âš ï¸ IMPORTANT: v0.0.1ì€ ì˜¤ë˜ëœ íƒœê·¸ì…ë‹ˆë‹¤. ìµœì‹  stable ì»¤ë°‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.

  1. Clone llama.cpp and checkout latest stable:
     git clone https://github.com/ggerganov/llama.cpp /tmp/llama.cpp
     cd /tmp/llama.cpp
     # ìµœì‹  stable tag ìë™ ì„ íƒ (ì˜ˆ: b3670 ì´ìƒ):
     LATEST_TAG=$(git tag --sort=-version:refname | grep '^b[0-9]' | head -1)
     git checkout $LATEST_TAG

  2. Build with CMake (Jetson Orin = Ampere SM87):
     cmake -B build \
           -DGGML_CUDA=ON \
           -DCMAKE_CUDA_ARCHITECTURES="87" \
           -DLLAMA_CURL=ON \
           -DCMAKE_BUILD_TYPE=Release \
           -DLLAMA_NATIVE=OFF \
           -DGGML_CUDA_FORCE_MMQ=ON \
           -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=ON
     # â†‘ Jetson í•µì‹¬ ìµœì í™”: Orinì€ CPU/GPUê°€ í†µí•© ë©”ëª¨ë¦¬(Unified Memory)ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.
     #   ì´ ì˜µì…˜ì„ ì¼œë©´ llama.cppê°€ CPUâ†”GPU ê°„ ë©”ëª¨ë¦¬ ë³µì‚¬ë¥¼ ìƒëµí•˜ê³ 
     #   ë‹¨ì¼ ë©”ëª¨ë¦¬ í’€ì„ ì§ì ‘ ì°¸ì¡° â†’ mmap + SIGSTOP/SIGCONT ì¡°í•©ê³¼ ì‹œë„ˆì§€ ê·¹ëŒ€í™”
     cmake --build build --config Release -j$(nproc)

  3. Install binaries to /usr/local/bin/:
     llama-server, llama-cli, llama-embedding

  4. Install llama.cpp C shared library to /usr/local/lib/:
     libllama.so, libggml.so, libggml-cuda.so (from build/src/ and build/ggml/src/)

  5. Install C headers to /usr/local/include/llama/

  6. Run ldconfig && verify: llama-cli --version must succeed

  7. Cleanup: rm -rf /tmp/llama.cpp (source ì‚­ì œë¡œ ë ˆì´ì–´ í¬ê¸° ìµœì†Œí™”)

Block C â€” Piper TTS (C++ ONNX-based TTS, ~50MB RAM):
  1. Download piper pre-built binary for aarch64:
     https://github.com/rhasspy/piper/releases/
     Target: piper_linux_aarch64.tar.gz
  2. Extract to /opt/piper/
  3. Symlink: /usr/local/bin/piper -> /opt/piper/piper
  4. Download Korean voice model:
     ko_KR-kss-medium.onnx + ko_KR-kss-medium.onnx.json
     Place in /opt/rover/models/tts/
  5. Test (build-time smoke test):
     echo "ì•ˆë…•í•˜ì„¸ìš”" | piper --model /opt/rover/models/tts/ko_KR-kss-medium.onnx \
       --output_file /tmp/test.wav && rm /tmp/test.wav

[Constraints]
- CUDA_ARCHITECTURES must be "87" â€” Jetson Orin (Ampere SM). DO NOT use "8.7" format.
- llama.cpp must be built with -DLLAMA_CURL=ON for HTTP model serving
- Do NOT install Python bindings for llama.cpp (use llama-server HTTP API only)
- All downloads must use wget with --progress=dot:giga
- Add sha256 checksum verification for Piper binary
- Source build directories must be deleted after install to minimize layer size

[Output format]
- Dockerfile content for stage `ai-runtime` only
- Each block (A, B, C) in a separate RUN layer group with comments
- Include a HEALTHCHECK that tests:
  llama-cli --version && python3 -c "import onnxruntime as ort; assert ort.get_device()=='GPU'"
```

---

## ğŸ‘ï¸ Prompt 3: Dockerfile â€” ë¹„ì „ ë ˆì´ì–´ (TensorRT C++ Runtime + YOLO11n ë˜í¼)

```
[Context â€” Prompt 2ì˜ `ai-runtime` ìŠ¤í…Œì´ì§€ ìœ„ì— ì´ì–´ì„œ ì‘ì„±]
- Previous stage: `ai-runtime` (ONNX Runtime + llama.cpp + Piper)
- This stage adds TensorRT C++ runtime for YOLO11n real-time object detection
- Output: adds stage `vision-runtime` to `docker/Dockerfile`
- Key constraint: YOLO11n runs on TensorRT (Tensor Cores), consuming 0 RAM (GPU only)

âš ï¸ CRITICAL DESIGN DECISION â€” YOLO11n Engine ë³€í™˜ì€ ì´ ì´ë¯¸ì§€ì—ì„œ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:
  - Ultralytics YOLO export(format="engine")ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ PyTorchë¥¼ í•„ìˆ˜ë¡œ ìš”êµ¬í•©ë‹ˆë‹¤.
  - PyTorchë¥¼ ì´ ì´ë¯¸ì§€ì— ì„¤ì¹˜í•˜ë©´ 15GB ë¬¸ì œê°€ ì¬ë°œí•©ë‹ˆë‹¤.
  - í•´ê²°ì±…: TRT ì—”ì§„ ë³€í™˜(.pt â†’ .engine)ì€ ì¼íšŒì„± ì‘ì—…ìœ¼ë¡œ,
    Prompt 6ì˜ first_run_setup.shì—ì„œ ë³„ë„ì˜ ì¼íšŒìš© NGC ì»¨í…Œì´ë„ˆ
    (nvcr.io/nvidia/l4t-pytorch:r36.x.x-pth2.x-py3)ë¥¼ ë„ì›Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  - ì´ ì´ë¯¸ì§€ëŠ” ì´ë¯¸ ë³€í™˜ ì™„ë£Œëœ .engine íŒŒì¼ì„ ë¡œë”©í•˜ëŠ” C++ ì½”ë“œë§Œ í¬í•¨í•©ë‹ˆë‹¤.

[Task]
Write the THIRD STAGE (`FROM ai-runtime AS vision-runtime`) of the multi-stage Dockerfile.

Block A â€” TensorRT C++ ê°œë°œ í—¤ë” ì„¤ì¹˜ (JetPack 6 ë‚´ì¥ TRT í™œìš©):
  JetPack 6.xëŠ” í˜¸ìŠ¤íŠ¸ì— TensorRT 8.6ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
  ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ C++ ì½”ë“œë¥¼ ì»´íŒŒì¼í•˜ë ¤ë©´ í—¤ë”ì™€ ë§í¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ í•„ìš”í•©ë‹ˆë‹¤.

  1. apt-get install (no recommended):
     - libnvinfer8                   # TensorRT ëŸ°íƒ€ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬
     - libnvinfer-dev                # C++ í—¤ë” (nvinfer.h ë“±)
     - libnvinfer-plugin8            # TRT í”ŒëŸ¬ê·¸ì¸ ëŸ°íƒ€ì„
     - libnvinfer-plugin-dev         # í”ŒëŸ¬ê·¸ì¸ í—¤ë”
     - libnvonnxparsers8             # ONNX â†’ TRT íŒŒì„œ (ì„ íƒì )
     - libnvonnxparsers-dev
     - cuda-cupti-dev-12-2           # CUDA profiling (ì„ íƒì )

  2. Verify TRT headers exist:
     test -f /usr/include/aarch64-linux-gnu/NvInfer.h && echo "TRT headers OK"

  3. Verify TRT version: /usr/lib/aarch64-linux-gnu/libnvinfer.so.8 should exist

  NOTE: pip install tensorrt ë˜ëŠ” pip install ultralyticsëŠ” ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.
        PyTorch ì˜ì¡´ì„±ì´ ë”¸ë ¤ ë“¤ì–´ì˜µë‹ˆë‹¤.

Block B â€” YOLO11n C++ ì¶”ë¡  ë˜í¼ (TensorRT ì§ì ‘ í˜¸ì¶œ, PyTorch ì™„ì „ ë°°ì œ):
  Write a C++ header-only library at /opt/rover/include/yolo_trt.hpp that:
  1. Loads yolo11n_fp16.engine using TensorRT C++ API (NvInfer.h, NvInferRuntime.h)
  2. Engine path taken from env var: YOLO_ENGINE (default: /opt/rover/models/vision/yolo11n_fp16.engine)
  3. Exposes a simple inference function:
     struct Detection { float x, y, w, h, conf; int class_id; std::string class_name; };
     class YoloTRT {
       public:
         YoloTRT(const std::string& engine_path);
         std::vector<Detection> detect(const uint8_t* rgb_data, int width, int height,
                                        float conf_thresh = 0.5f, float nms_thresh = 0.45f);
       private:
         // TensorRT engine, context, CUDA buffers (pre-allocated at init)
     };
  4. Uses CUDA streams for async inference (cudaStreamCreate)
  5. Pre-allocates input/output CUDA buffers at constructor time (no per-frame malloc)
  6. Implements NMS (Non-Maximum Suppression) in CPU after GPU inference
  7. Target latency: < 10ms per 640x640 frame on Jetson Orin (125 FPS)
  8. COCO class names array included (80 classes)

Block C â€” ROS 2 Vision ë…¸ë“œ (depth_camera_cpp í†µí•©í˜•):
  Write a complete C++ ROS 2 node source at /opt/rover/src/yolo_ros_node.cpp that:
  1. Includes yolo_trt.hpp
  2. Node class: YoloDetectorNode : public rclcpp::Node
  3. Subscribes to /camera/color/image_raw (sensor_msgs/msg/Image, BGR8)
     - Converts ROS Image msg to uint8_t* (no cv_bridge PyTorch backend)
     - Uses only libopencv-dev (already installed in base)
  4. Publishes /vision/detections (std_msgs/msg/String, JSON)
     JSON format: {
       "timestamp": 1234567890.123,
       "frame_id": "camera_color_optical_frame",
       "detections": [{"class": "cup", "conf": 0.92, "bbox": [x,y,w,h], "center": [cx,cy]}]
     }
  5. Publishes /vision/image_annotated (sensor_msgs/msg/Image) with bounding boxes drawn
     (OpenCV rectangle + text, no PyTorch)
  6. Parameter: ~conf_threshold (default 0.5), ~engine_path

  Also write the CMakeLists.txt for this node that:
  - find_package: rclcpp, sensor_msgs, std_msgs, OpenCV
  - target_link_libraries: libnvinfer, libcudart (TensorRT + CUDA)
  - Copies yolo_trt.hpp to include/

[Constraints]
- NO pip install, NO Python code in this stage (C++ only for TRT)
- NO ultralytics, NO torch imports anywhere
- The .engine file is NOT baked into the image â€” it is mounted from host SSD at runtime
  via: -v /home/ubuntu/AI_secretary_robot/models/vision:/opt/rover/models/vision:ro
- The C++ wrapper must use raw CUDA buffers, NOT torch::Tensor
- NvInfer.h must be found at build time (verify in Dockerfile RUN)

[Output format]
- Dockerfile content for stage `vision-runtime`
- Complete /opt/rover/include/yolo_trt.hpp
- Complete /opt/rover/src/yolo_ros_node.cpp
- Complete /opt/rover/src/CMakeLists_yolo.txt (for the yolo node)
```

---

## ğŸ¤– Prompt 4: Dockerfile â€” ROS 2 Heavy íŒ¨í‚¤ì§€ + workspace ë¹Œë“œ

```
[Context â€” Prompt 3ì˜ `vision-runtime` ìŠ¤í…Œì´ì§€ ìœ„ì— ì´ì–´ì„œ ì‘ì„±]
- Previous stage: `vision-runtime` (ONNX Runtime + llama.cpp + Piper + TRT C++ headers)
- ROS 2 Humble base (ros-humble-ros-base)ëŠ” Prompt 1ì—ì„œ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- ì´ ìŠ¤í…Œì´ì§€ì—ì„œëŠ” ROS 2ì˜ ë¬´ê±°ìš´ íŒ¨í‚¤ì§€(MoveIt2, Nav2)ì™€ workspace ë¹Œë“œë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- Output: adds stage `ros2-runtime` to `docker/Dockerfile`

[Existing workspace C++ packages to build â€” located at /home/ubuntu/AI_secretary_robot/src/]:
  - wake_vad_cpp      # Wake word + VAD (openwakeword, webrtcvad)
  - stt_cpp           # Moonshine ONNX STT node (libcurl for model API)
  - intent_router_cpp # KoSimCSE ONNX intent classifier
  - llm_cpp           # LLM node (calls llama-server via HTTP/curl)
  - tts_cpp           # TTS node (Piper via subprocess + ALSA playback)
  - ros_robot_controller_msgs  # Custom message types
  - ros_robot_controller_cpp   # Hardware controller (UART/USB)
  - jetrover_arm_moveit         # MoveIt 2 arm planning
  - depth_camera_cpp            # Orbbec Dabai DCW (USB 2.0 RGB-D)
  - imu_bridge_cpp              # MPU6050 IMU bridge
  - battery_cpp                 # Battery monitor
  - rover_autonomy_cpp          # Nav2 + SLAM integration
  - rover_common                # Shared utilities

[Task]
Write the FOURTH STAGE (`FROM vision-runtime AS ros2-runtime`) of the Dockerfile.

Block A â€” ROS 2 Heavy íŒ¨í‚¤ì§€ ì¶”ê°€ ì„¤ì¹˜:
  âš ï¸ ros-humble-ros-baseëŠ” ì´ë¯¸ Prompt 1(base stage)ì— ì„¤ì¹˜ë¨. ì¤‘ë³µ ì„¤ì¹˜ ê¸ˆì§€.
  âš ï¸ rosdep initë„ ì´ë¯¸ ì™„ë£Œ. rosdep updateë§Œ ì¬ì‹¤í–‰.

  1. apt-get install (ì¶”ê°€ íŒ¨í‚¤ì§€ë§Œ):
     - ros-humble-moveit                    # MoveIt 2 (íŒ” ì œì–´)
     - ros-humble-nav2-bringup              # Nav2 ì „ì²´ ìŠ¤íƒ
     - ros-humble-slam-toolbox              # SLAM (ì§€ë„ ì‘ì„±)
     - ros-humble-trac-ik-kinematics-plugin # IK ì†”ë²„ (ì—­ê¸°êµ¬í•™)
     - ros-humble-ros2-control              # í•˜ë“œì›¨ì–´ ì¸í„°í˜ì´ìŠ¤
     - ros-humble-ros2-controllers          # ëª¨í„° ì»¨íŠ¸ë¡¤ëŸ¬
     - ros-humble-joint-state-publisher     # URDF ê´€ì ˆ ìƒíƒœ

  2. Orbbec Dabai DCW ë“œë¼ì´ë²„ ì„¤ì¹˜:
     ë°©ë²• A (apt íŒ¨í‚¤ì§€ê°€ ìˆëŠ” ê²½ìš° ì‹œë„):
       apt-get install ros-humble-orbbec-camera || BUILD_FROM_SOURCE=true

     ë°©ë²• B (apt ì‹¤íŒ¨ ì‹œ ì†ŒìŠ¤ ë¹Œë“œ â€” fallback):
       git clone https://github.com/orbbec/OrbbecSDK_ROS2.git /tmp/orbbec_ros2
       cd /tmp/orbbec_ros2
       source /opt/ros/humble/setup.bash
       colcon build --base-paths . --cmake-args -DCMAKE_BUILD_TYPE=Release
       cp -r install/* /opt/ros/humble/
       rm -rf /tmp/orbbec_ros2

     âš ï¸ ì†ŒìŠ¤ ë¹Œë“œ ì‹œ /opt/rover/ws/srcì— í¬í•¨í•˜ì§€ ë§ê³  /opt/ros/humble/ì— ì§ì ‘ ì„¤ì¹˜.
        ì´ë ‡ê²Œ í•´ì•¼ source /opt/ros/humble/setup.bash í•œ ë²ˆìœ¼ë¡œ ìë™ ì¸ì‹.

Block B â€” workspace ë¹Œë“œ:
  1. COPY the entire workspace src/ tree into /opt/rover/ws/src/
     (At build time: COPY --chown=root:root ./src /opt/rover/ws/src)

  2. Update rosdep (initì€ ì´ë¯¸ ì™„ë£Œ):
     rosdep update --rosdistro humble

  3. Install rosdep dependencies for workspace:
     source /opt/ros/humble/setup.bash
     cd /opt/rover/ws
     rosdep install --from-paths src --ignore-src -r -y \
       --skip-keys "orbbec_camera orbbec_camera_msgs"
     # orbbecëŠ” Block Aì—ì„œ ë³„ë„ ì„¤ì¹˜í–ˆìœ¼ë¯€ë¡œ skip

  4. Build with colcon:
     source /opt/ros/humble/setup.bash
     cd /opt/rover/ws
     colcon build \
       --base-paths src \
       --cmake-args \
         -DCMAKE_BUILD_TYPE=Release \
         -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \
       --parallel-workers $(nproc) \
       --event-handlers console_direct+

  5. Source the install:
     echo "source /opt/rover/ws/install/setup.bash" >> /etc/bash.bashrc

  6. Remove build artifacts to reduce layer size:
     rm -rf /opt/rover/ws/build /opt/rover/ws/log

Block C â€” ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ì„¤ì •:
  1. Create ALSA config at /etc/asound.conf:
     - Set default PCM to ReSpeaker 6-Mic Array (USB audio, vendor 2886:0018)
     - Set default CTL for volume control
     - Add a loopback device for TTS playback monitoring
  2. Add udev rule for ReSpeaker: /etc/udev/rules.d/99-respeaker.rules
     SUBSYSTEM=="sound", ATTRS{idVendor}=="2886", ATTRS{idProduct}=="0018", \
     SYMLINK+="respeaker", MODE="0666"
  3. Add udev rule for Orbbec Dabai DCW: /etc/udev/rules.d/99-orbbec.rules
     SUBSYSTEM=="usb", ATTRS{idVendor}=="2bc5", MODE="0666", GROUP="plugdev"
     (Orbbec USB vendor ID: 0x2bc5)

[Constraints]
- ros-humble-ros-base ì¬ì„¤ì¹˜ ê¸ˆì§€ (ì´ë¯¸ Prompt 1 base stageì— ìˆìŒ)
- colcon build: --cmake-args -DCMAKE_BUILD_TYPE=Release (debug symbols off)
- Build/log ë””ë ‰í† ë¦¬ ì‚­ì œë¡œ ì´ë¯¸ì§€ ë ˆì´ì–´ í¬ê¸° ìµœì†Œí™”
- model íŒŒì¼ì€ ì´ë¯¸ì§€ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (host SSDì—ì„œ bind-mount)
- ì™„ì„±ëœ install/ ê²°ê³¼ë¬¼ì€ ì´ë¯¸ì§€ ë ˆì´ì–´ì— í¬í•¨ë¨

[Output format]
- Dockerfile content for stage `ros2-runtime`
- /etc/asound.conf (ALSA config for ReSpeaker 6-mic + 3W speaker)
- /etc/udev/rules.d/99-respeaker.rules
- /etc/udev/rules.d/99-orbbec.rules
- Comment at top: expected final image size (target: < 4GB total)
```

---

## ğŸ”€ Prompt 5: Python ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° â€” 3-Track íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €

```
[Context]
ì´ë¯¸ êµ¬ì¶•ëœ Docker ì´ë¯¸ì§€ ì•ˆì—ì„œ ì‹¤í–‰ë  ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª¨ë“  ROS 2 ë…¸ë“œë¥¼ ê´€ë¦¬í•˜ê³ , 3-Track ë¶„ê¸° ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

[Task]
Write `/opt/rover/scripts/pipeline_manager.py` â€” the main orchestration process.

This script runs INSIDE the container and manages the full data pipeline:

=== ARCHITECTURE ===

Always-ON (ìƒì‹œ ëŒ€ê¸°ì¡°, RAM < 300MB total):
  - wake_vad_cpp node   â†’ detects wake word "ìë¹„ìŠ¤"
  - stt_cpp node        â†’ Moonshine ONNX, transcribes after wake word
  - intent_router_cpp   â†’ KoSimCSE ONNX, classifies intent (Track A/B/C)
  - tts_cpp node        â†’ Piper TTS, always listening for /tts/text topic
  - yolo_ros_node       â†’ YOLO11n TRT, always processing camera frames (Track A)

On-Demand (ìˆ˜ìš” ëŒ€ê¸°ì¡°, SSD mmap â†’ RAM):
  - llama-server (LLM)  â†’ Qwen 1.5B GGUF, loaded only for Track B/C
  - llama-server (VLM)  â†’ Moondream 1.8B GGUF, loaded only for Track B

=== TRACK ROUTING LOGIC ===

The intent_router_cpp publishes to /intent/classification with JSON:
  {"track": "A"|"B"|"C", "confidence": 0.95, "utterance": "...", "params": {...}}

Track A â€” Fast Path (simple navigation/manipulation):
  Triggers: navigate, pick, place, move, go, stop
  Flow: intent â†’ YOLO detection â†’ coordinate extraction â†’ /arm/goal or /nav/goal
  Latency target: < 200ms total
  LLM/VLM: STAY ASLEEP

Track B â€” VLM Track (visual complex reasoning):
  Triggers: describe, find by color/shape, "which one", "what is"
  Flow: intent â†’ [wake VLM] â†’ capture frame â†’ VLM inference â†’ coordinate â†’ arm/nav
  Latency target: < 3s total (VLM load: ~1s, inference: ~2s)
  VLM lifecycle:
    1. Send SIGCONT to llama-server (VLM) process â†’ it mmap-loads from SSD
    2. POST to http://localhost:8081/v1/chat/completions with image+text
    3. Parse response for coordinates/actions
    4. After response: send SIGSTOP to llama-server (VLM) â†’ returns SSD
    NOTE: Use SIGSTOP/SIGCONT for instant freeze/resume (no restart overhead)

Track C â€” LLM Track (conversation/questions):
  Triggers: question, explain, what, why, how, weather, schedule
  Flow: intent â†’ [wake LLM] â†’ text inference â†’ TTS response
  Latency target: < 1.5s first token
  LLM lifecycle: same SIGSTOP/SIGCONT pattern as VLM

=== PROCESS MANAGEMENT ===

class LlamaServerManager:
  """Manages llama-server processes with SIGSTOP/SIGCONT for zero-restart lazy loading."""

  def __init__(self, model_path: str, port: int, ctx_size: int = 2048):
      self.model_path = model_path   # GGUF file on NVMe SSD (bind-mounted)
      self.port = port
      self.ctx_size = ctx_size
      self.process: subprocess.Popen = None
      self.state = "stopped"  # stopped | sleeping | running

  def start(self):
      """Start llama-server with mmap enabled (models stay on SSD until needed)."""
      cmd = [
          "llama-server",
          "--model", self.model_path,
          "--port", str(self.port),
          "--ctx-size", str(self.ctx_size),
          "--n-gpu-layers", "99",   # All layers to GPU
          "--mmap",                  # KEY: memory-mapped file (lazy load from SSD)
          "--no-mlock",              # Allow OS to page out mmap pages â†’ RAM ì ˆì•½
          "--threads", "4",
          "--batch-size", "512",
          "--host", "127.0.0.1",
      ]
      self.process = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
      # Wait for server ready: poll GET http://localhost:{port}/health
      self._wait_for_health()
      self.state = "sleeping"
      os.kill(self.process.pid, signal.SIGSTOP)  # Freeze immediately after ready

  def wake(self):
      """Resume the frozen process (mmap pages load from SSD on demand)."""
      if self.state == "sleeping":
          os.kill(self.process.pid, signal.SIGCONT)
          self.state = "running"
          self._wait_for_health()  # Re-check health after SIGCONT

  def sleep(self):
      """Freeze the process (OS can evict mmap pages to reclaim RAM)."""
      if self.state == "running":
          os.kill(self.process.pid, signal.SIGSTOP)
          self.state = "sleeping"

  def _wait_for_health(self, timeout: int = 30):
      """Poll /health endpoint until server is ready."""
      # HTTP GET http://127.0.0.1:{port}/health, retry every 0.5s, timeout after 30s

=== MEMORY MANAGEMENT ===

Memory budget (8GB total):
  Always-ON stack:    ~300MB RAM
  ROS 2 + Nav2:      ~1.5GB RAM
  SLAM + MoveIt:     ~1.0GB RAM
  LLM (when active): ~0.8GB RAM (Qwen 1.5B Q4_K_M, GPU layers)
  VLM (when active): ~0.9GB RAM (Moondream 1.8B GGUF, GPU layers)
  Free buffer:       ~3.5GB (no OOM)

Add a memory watchdog thread that:
  - Polls /proc/meminfo every 5 seconds
  - If MemAvailable < 512MB: force-sleep both LLM and VLM servers, log warning
  - Publishes /system/memory_status (std_msgs/msg/String, JSON)
  JSON: {"mem_total_mb": 8192, "mem_available_mb": 3500, "llm_state": "sleeping", ...}

=== ROS 2 INTEGRATION ===

The orchestrator must:
  1. source /opt/ros/humble/setup.bash and /opt/rover/ws/install/setup.bash before launch
  2. Launch all always-ON nodes via subprocess (ros2 run ...)
  3. Subscribe to /intent/classification via rclpy (in a dedicated thread)
  4. Route to Track A/B/C based on classification
  5. Publish action goals to MoveIt (/arm/goal) and Nav2 (/nav/goal)
  6. Error handling: if Track B VLM fails â†’ fallback to Track A (YOLO only)
  7. On SIGTERM: send SIGTERM to all child processes, wait, then exit cleanly

=== DATABASE INTEGRATION ===

Use sqlite3 (stdlib, thread-safe mode) to log every interaction:
  - Log conversation to `conversations` table (see schema in README.md)
  - Update `objects` table when YOLO/VLM detects a new object
  - Update `object_locations` when arm successfully picks an object
  DB path from env: ROVER_DB_PATH (default: /opt/rover/db/rover.db)

[Output]
- /opt/rover/scripts/pipeline_manager.py (full implementation, ~400 lines)
- /opt/rover/scripts/launch_all.sh (bash script to start the full pipeline)
  This script must:
    1. source /opt/ros/humble/setup.bash
    2. source /opt/rover/ws/install/setup.bash
    3. export all required env vars (ROVER_MODEL_DIR, ROVER_DB_PATH, etc.)
    4. exec python3 /opt/rover/scripts/pipeline_manager.py
    5. Trap SIGTERM/SIGINT to trigger clean shutdown
```

---

## ğŸš€ Prompt 6: Docker Compose + ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ì¡°ë¦½)

```
[Context]
ëª¨ë“  Dockerfile ìŠ¤í…Œì´ì§€(Prompt 1~4)ì™€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°(Prompt 5)ê°€ ì™„ì„±ëœ ìƒíƒœì…ë‹ˆë‹¤.
ì´ì œ ì „ì²´ ì‹œìŠ¤í…œì„ í•˜ë‚˜ì˜ ëª…ë ¹ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” docker-compose.ymlê³¼ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

Host environment:
  - Host OS: Ubuntu 22.04, JetPack 6.x
  - Docker runtime: nvidia-container-runtime (already configured)
  - Models on host NVMe: /home/ubuntu/AI_secretary_robot/models/
  - rover_ws on host: /home/ubuntu/rover_ws/
  - Audio: ReSpeaker 6-Mic USB (vendor 2886:0018), 3W speaker via 3.5mm jack
  - Camera: Orbbec Dabai DCW (USB 2.0, vendor 2bc5)
  - Robot: /dev/ttyUSB0 (UART to robot controller)

[Task]

--- File 1: docker-compose.yml ---
Write a docker-compose.yml with ONE service: `rover`

service `rover`:
  image: jetrover/rover:latest
  container_name: jetrover_rover
  restart: unless-stopped

  runtime: nvidia
  environment:
    - NVIDIA_VISIBLE_DEVICES=all
    - NVIDIA_DRIVER_CAPABILITIES=compute,utility,video
    - ROS_DOMAIN_ID=42
    - RMW_IMPLEMENTATION=rmw_fastrtps_cpp
    - FASTRTPS_DEFAULT_PROFILES_FILE=/opt/rover/config/fastdds.xml
    - PULSE_SERVER=unix:/run/user/1000/pulse/native
    - ROVER_MODEL_DIR=/opt/rover/models
    - ROVER_DB_PATH=/opt/rover/db/rover.db
    - LLM_MODEL=qwen2.5-1.5b-instruct-q4_k_m.gguf
    - VLM_MODEL=moondream2-1.8b-f16.gguf
    - YOLO_ENGINE=/opt/rover/models/vision/yolo11n_fp16.engine

  volumes:
    # Models on NVMe SSD (read-only, mmap from here)
    - /home/ubuntu/AI_secretary_robot/models:/opt/rover/models:ro
    # Persistent DB (read-write, bind-mounted from host)
    - /home/ubuntu/AI_secretary_robot/db:/opt/rover/db:rw
    # ROS 2 workspace install (host build syncìš©, read-only)
    # NOTE: ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì— ì´ë¯¸ ë¹Œë“œëœ /opt/rover/ws/installì´ ìˆìœ¼ë‚˜,
    #       hostì—ì„œ ì¬ë¹Œë“œ í›„ ë™ê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš° ì•„ë˜ ì£¼ì„ì„ í•´ì œ:
    # - /home/ubuntu/rover_ws/install:/opt/rover/ws/install:ro
    # Audio (PulseAudio socket)
    - /run/user/1000/pulse:/run/user/1000/pulse

  devices:
    - /dev/snd:/dev/snd           # ALSA raw audio
    - /dev/ttyUSB0:/dev/ttyUSB0  # Robot UART controller

  device_cgroup_rules:
    - 'c 189:* rmw'   # USB devices (ReSpeaker mic, Orbbec camera)
    - 'c 81:* rmw'    # Video devices (V4L2)

  network_mode: host   # ROS 2 DDS requires host networking (ROS_DOMAIN_ID ê³µìœ )

  ipc: host            # Shared memory for ROS 2 zero-copy transport

  ulimits:
    memlock: -1
    stack: 67108864

  privileged: false
  cap_add:
    - SYS_NICE          # For real-time scheduling (ROS 2 executor)
    - NET_ADMIN         # For ROS 2 DDS multicast

  command: /opt/rover/scripts/launch_all.sh

  healthcheck:
    test: ["CMD", "bash", "-c",
           "source /opt/ros/humble/setup.bash && ros2 node list | grep -q stt_node"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 90s   # MoveIt ë¡œë”© ì‹œê°„ ê³ ë ¤


--- File 2: scripts/build_image.sh ---
Write a bash script to build the final Docker image:
  1. cd to project root (where docker/Dockerfile lives)
  2. Build:
     docker build --platform linux/arm64 \
       --build-arg JETPACK_VERSION=36.4.3 \
       -t jetrover/rover:latest \
       -f docker/Dockerfile \
       --progress=plain \
       .
  3. Print image size: docker image inspect jetrover/rover:latest --format='{{.Size}}'
  4. Smoke test:
     docker run --rm --runtime nvidia jetrover/rover:latest \
       bash -c "source /opt/ros/humble/setup.bash && \
                ros2 --version && \
                llama-cli --version && \
                python3 -c 'import onnxruntime as ort; assert ort.get_device()==\"GPU\"'"
  5. If --export flag given: docker save jetrover/rover:latest -o /tmp/rover_image.tar


--- File 3: scripts/first_run_setup.sh ---
Write a bash script for first-time setup on the Jetson HOST (not inside container):
  1. Check prerequisites: docker, nvidia-container-runtime, nvcc

  2. Create host directories:
       /home/ubuntu/AI_secretary_robot/models/llm/
       /home/ubuntu/AI_secretary_robot/models/vlm/
       /home/ubuntu/AI_secretary_robot/models/vision/
       /home/ubuntu/AI_secretary_robot/db/

  3. Download YOLO11n weights (.pt file) on HOST (not in container):
       pip3 install ultralytics --quiet
       python3 -c "from ultralytics import YOLO; YOLO('yolo11n.pt')"
       find ~/.cache/ultralytics -name "yolo11n.pt" -exec cp {} \
         /home/ubuntu/AI_secretary_robot/models/vision/ \;

  4. âš ï¸ YOLO11n â†’ TensorRT Engine ë³€í™˜ (ì¼íšŒì„± ì‘ì—…):
     ì¼íšŒìš© l4t-pytorch ì»¨í…Œì´ë„ˆë¥¼ ë„ì›Œ ë³€í™˜ ìˆ˜í–‰ (rover ì´ë¯¸ì§€ì—ëŠ” PyTorch ì—†ìŒ):

     docker run --rm --runtime nvidia \
       -v /home/ubuntu/AI_secretary_robot/models:/models \
       nvcr.io/nvidia/l4t-pytorch:r36.4.3-pth2.4-py3 \
     # â†‘ JetPack 6.0+ ìµœì‹  ê³µì‹ íƒœê·¸: r36.4.3-pth2.4-py3 (PyTorch 2.4, Python 3.10)
     #   fallback: r36.3.0-pth2.3-py3 (íƒœê·¸ í™•ì¸: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)
       python3 -c "
         from ultralytics import YOLO
         import os
         model = YOLO('/models/vision/yolo11n.pt')
         model.export(
           format='engine',
           device=0,
           half=True,
           workspace=2,
           simplify=True,
           imgsz=640
         )
         import shutil
         shutil.move('/models/vision/yolo11n.engine',
                     '/models/vision/yolo11n_fp16.engine')
         print('Engine saved to /models/vision/yolo11n_fp16.engine')
         print('Size:', os.path.getsize('/models/vision/yolo11n_fp16.engine'), 'bytes')
       "

     âš ï¸ ì´ ë‹¨ê³„ëŠ” ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰. ì™„ë£Œ í›„ l4t-pytorch ì»¨í…Œì´ë„ˆëŠ” ìë™ ì‚­ì œë¨(--rm).

  5. Initialize SQLite DB schema:
     docker run --rm \
       -v /home/ubuntu/AI_secretary_robot/db:/opt/rover/db \
       jetrover/rover:latest \
       python3 /opt/rover/scripts/init_db.py

  6. Print summary and next steps:
     "âœ… Setup complete!"
     "Engine: /home/ubuntu/AI_secretary_robot/models/vision/yolo11n_fp16.engine"
     "DB: /home/ubuntu/AI_secretary_robot/db/rover.db"
     "Run: docker-compose up -d"


--- File 4: scripts/init_db.py ---
Write a Python script to initialize the SQLite database schema:
  1. DB path from env ROVER_DB_PATH (default: /opt/rover/db/rover.db)
  2. Use the full schema defined in README.md:
     conversations, objects, locations, object_locations,
     system_logs, users, task_queue, embedding_cache
  3. Create SQLite FTS5 virtual table for full-text search:
     CREATE VIRTUAL TABLE conversations_fts USING fts5(user_utterance, tts_response, ...)
  4. Create indexes: idx_timestamp, idx_session, idx_intent
  5. Insert default data:
     - 5 common Korean household objects in `objects` table
     - 1 default user profile in `users` table
  6. Print: "âœ… Database initialized: <path> â€” <N> tables created"


[Output]
- docker-compose.yml
- docker/Dockerfile final stage: `FROM ros2-runtime AS final`
  COPY scripts/ /opt/rover/scripts/
  RUN chmod +x /opt/rover/scripts/*.sh /opt/rover/scripts/*.py
  EXPOSE 8080   # llama-server LLM (Qwen)
  EXPOSE 8081   # llama-server VLM (Moondream)
  EXPOSE 8765   # AI WebSocket API (optional monitoring)
  WORKDIR /opt/rover
  ENTRYPOINT ["/opt/rover/scripts/launch_all.sh"]
- scripts/build_image.sh
- scripts/first_run_setup.sh  (âš ï¸ YOLO TRT ë³€í™˜ì€ l4t-pytorch ì¼íšŒìš© ì»¨í…Œì´ë„ˆ ì‚¬ìš©)
- scripts/init_db.py
```

---

## ğŸ“‹ ì‹¤í–‰ ìˆœì„œ ìš”ì•½

| ë‹¨ê³„ | í”„ë¡¬í”„íŠ¸ | ì‚°ì¶œë¬¼ | ì˜ˆìƒ ìš©ëŸ‰ |
|:---:|:---|:---|---:|
| 1 | Base + ROS 2 | `Dockerfile` (base stage) | ~2.0GB |
| 2 | AI Runtime | `Dockerfile` (ai-runtime stage) | +800MB |
| 3 | Vision (TRT C++) | `Dockerfile` (vision-runtime stage) | +200MB |
| 4 | MoveIt/Nav2 + ë¹Œë“œ | `Dockerfile` (ros2-runtime stage) | +1.0GB |
| 5 | Orchestrator | `pipeline_manager.py`, `launch_all.sh` | - |
| 6 | Deploy | `docker-compose.yml`, ë¹Œë“œ/ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ | - |
| **í•©ê³„** | | **ìµœì¢… ì´ë¯¸ì§€** | **~4GB** |

> ê¸°ì¡´ l4t-pytorch ì´ë¯¸ì§€(15GB) ëŒ€ë¹„ **ì•½ 73% ì ˆê°**
> YOLO11n TRT ë³€í™˜ì€ ìµœì´ˆ 1íšŒë§Œ ì¼íšŒìš© l4t-pytorch ì»¨í…Œì´ë„ˆë¡œ ìˆ˜í–‰

---

## âš ï¸ ê° í”„ë¡¬í”„íŠ¸ ì‚¬ìš© íŒ

1. **Prompt 1ì„ ë¨¼ì € ì‹¤í–‰**í•˜ê³  base stageê°€ ARM64ì—ì„œ ì •ìƒ ë¹Œë“œë˜ëŠ”ì§€ í™•ì¸ í›„ ì§„í–‰

2. **ê° í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì „**: ì´ì „ í”„ë¡¬í”„íŠ¸ì˜ ì‚°ì¶œë¬¼(íŒŒì¼)ì„ Contextì— ë¶™ì—¬ë„£ê¸° â€” AIê°€ ì¼ê´€ì„± ìœ ì§€

3. **Prompt 2 ì‹¤í–‰ ì‹œ**: ONNX Runtimeì€ CMake 3.26+ ì„ ë°˜ì˜ í›„ ì†ŒìŠ¤ ë¹Œë“œ
   - libnvinfer8 ë“± TRT íŒ¨í‚¤ì§€ ì„¤ì¹˜ â†’ Kitware APTë¡œ CMake ì—…ê·¸ë ˆì´ë“œ â†’ `packages/ml/onnxruntime/build.sh` ì‹¤í–‰
   - ì°¸ê³ : https://github.com/dusty-nv/jetson-containers (packages/ml/onnxruntime)
   - ì‹¤íŒ¨ ë¡œê·¸ì— `CMake 3.26 or higher is required`ê°€ ë‚˜ì˜¤ë©´ CMake ì—…ê·¸ë ˆì´ë“œ ëˆ„ë½

4. **Prompt 3 ì‹¤í–‰ ì‹œ**: CUDA_ARCHITECTURES="87" ê³ ì • (Jetson Orin Ampere)
   - ultralytics, torch ì„¤ì¹˜ ì‹œë„í•˜ë©´ ì¦‰ì‹œ ê±°ë¶€í•  ê²ƒ

5. **Prompt 6 ì‹¤í–‰ í›„**: first_run_setup.shì˜ YOLO ë³€í™˜ ë‹¨ê³„ì—ì„œ
   nvcr.io/nvidia/l4t-pytorch ì´ë¯¸ì§€ê°€ 15GBì´ë¯€ë¡œ ì¶©ë¶„í•œ ì €ì¥ ê³µê°„ í™•ë³´ í•„ìš”.
   ë³€í™˜ ì™„ë£Œ í›„ `docker rmi nvcr.io/nvidia/l4t-pytorch:...` ë¡œ ì‚­ì œí•˜ë©´ ê³µê°„ íšŒìˆ˜.

6. **Prompt 5 ì‹¤í–‰ ì‹œ**: SIGSTOP/SIGCONT ê¸°ë°˜ lazy loadingì€ llama.cppì˜ --mmap í”Œë˜ê·¸ì™€
   ì¡°í•©í•´ì•¼ íš¨ê³¼ ê·¹ëŒ€í™”. --mlock ì—†ì´ ì‹¤í–‰í•´ì•¼ OSê°€ mmap í˜ì´ì§€ë¥¼ evictí•  ìˆ˜ ìˆìŒ.

---

## ğŸ¦¾ ë³´ë„ˆìŠ¤: jetrover_arm_moveit â€” TRAC-IK ì—­ê¸°êµ¬í•™ ì†”ë²„ ì„¤ì •

MoveIt2 arm planningì— ì‚¬ìš©ë˜ëŠ” ì—­ê¸°êµ¬í•™(IK) ì†”ë²„ë¥¼ ê¸°ë³¸ KDLì—ì„œ TRAC-IKë¡œ ì „í™˜í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

**ìƒì„¸ í”„ë¡¬í”„íŠ¸ ë¬¸ì„œ**: [`docs/codex_prompt_trac_ik.md`](./codex_prompt_trac_ik.md)

**Quick Setup**:
```bash
# 1. TRAC-IK í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ (ì´ë¯¸ package.xmlì— ì˜ì¡´ì„± í¬í•¨ë¨)
sudo apt install -y ros-humble-trac-ik-kinematics-plugin

# 2. ì´ë¯¸ ì ìš©ëœ ì„¤ì • í™•ì¸
cat src/control/jetrover_arm_moveit/config/kinematics.yaml

# 3. ë¹Œë“œ ë° í…ŒìŠ¤íŠ¸
cd /home/ubuntu/AI_secretary_robot
source /opt/ros/humble/setup.bash
colcon build --packages-select jetrover_arm_moveit
source install/setup.bash
ros2 launch jetrover_arm_moveit moveit_demo.launch.py
```

**ì£¼ìš” ë³€ê²½ì‚¬í•­**:
- `kinematics_solver`: `kdl_kinematics_plugin/KDLKinematicsPlugin` â†’ `trac_ik_kinematics_plugin/TRAC_IKKinematicsPlugin`
- `solve_type`: `Speed` (ì‹¤ì‹œê°„ ì œì–´ ìµœì í™”)
- `kinematics_solver_timeout`: 0.05ì´ˆ â†’ 0.01ì´ˆ (10ms, Jetson Orin ìµœì í™”)
- `position_tolerance`: 0.001m (1mm ì •ë°€ë„)
- `orientation_tolerance`: 0.05 rad (~3Â° ì •ë°€ë„)

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**:
- IK ê³„ì‚° ì‹œê°„: 15~30ms (KDL) â†’ 3~5ms (TRAC-IK Speed)
- ì„±ê³µë¥  (ì¼ë°˜): 60~70% â†’ 85~95%
- ì„±ê³µë¥  (íŠ¹ì´ì  ê·¼ì²˜): 10~20% â†’ 60~80%

**ìƒì„¸ íŒŒë¼ë¯¸í„° íŠœë‹ ë° Troubleshooting**: [`codex_prompt_trac_ik.md`](./codex_prompt_trac_ik.md) ì°¸ì¡°
