#!/bin/bash
# LLM ì˜ì¡´ì„± ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
# ì‘ì„±ì¼: 2026-03-01
# ìš©ë„: Ollama, llama.cpp, Qwen2.5 ëª¨ë¸ ìë™ ì„¤ì¹˜

set -e  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ

echo "========================================="
echo "LLM ì˜ì¡´ì„± ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸"
echo "========================================="
echo ""

# ============================================
# 1. Ollama ì„¤ì¹˜ (ì¦‰ì‹œ fallback ë³µêµ¬)
# ============================================
echo "[1/3] Ollama ì„¤ì¹˜ ì¤‘..."
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama ì´ë¯¸ ì„¤ì¹˜ë¨: $(ollama --version)"
else
    echo "ğŸ“¦ Ollama ê³µì‹ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘..."
    curl -fsSL https://ollama.com/install.sh | sh
    echo "âœ… Ollama ì„¤ì¹˜ ì™„ë£Œ"
fi

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
echo "ğŸš€ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘..."
sudo systemctl start ollama || echo "âš ï¸  systemd ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨, ìˆ˜ë™ ì‹¤í–‰ í•„ìš”"
sudo systemctl enable ollama || true

# Ollama ì¤€ë¹„ ëŒ€ê¸°
echo "â³ Ollama ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
for i in {1..10}; do
    if curl -s http://localhost:11434/api/tags &> /dev/null; then
        echo "âœ… Ollama ì„œë²„ ì¤€ë¹„ ì™„ë£Œ"
        break
    fi
    sleep 1
done

# Qwen2.5-1.5B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
echo "ğŸ“¥ Qwen2.5:1.5b ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 1.2GB, ìˆ˜ ë¶„ ì†Œìš”)..."
if ollama list | grep -q "qwen2.5:1.5b"; then
    echo "âœ… Qwen2.5:1.5b ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨"
else
    ollama pull qwen2.5:1.5b
    echo "âœ… Qwen2.5:1.5b ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
fi

# Ollama í…ŒìŠ¤íŠ¸
echo "ğŸ§ª Ollama ì¶”ë¡  í…ŒìŠ¤íŠ¸..."
RESPONSE=$(ollama run qwen2.5:1.5b "ì•ˆë…•" --verbose 2>&1 | head -20)
if echo "$RESPONSE" | grep -q "ì•ˆë…•"; then
    echo "âœ… Ollama ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ"
else
    echo "âš ï¸  Ollama ì‘ë‹µ: $RESPONSE"
fi

echo ""

# ============================================
# 2. llama.cpp ë¹Œë“œ (Planning ë¬¸ì„œ ì¤€ìˆ˜)
# ============================================
echo "[2/3] llama.cpp ë¹Œë“œ ì¤‘..."

LLAMA_DIR="/home/sang/dev_ws/AI_secretary_robot/external/llama.cpp"

if [ -f "$LLAMA_DIR/build/bin/llama-server" ]; then
    echo "âœ… llama-server ì´ë¯¸ ë¹Œë“œë¨: $LLAMA_DIR/build/bin/llama-server"
else
    # ì˜ì¡´ì„± ì„¤ì¹˜
    echo "ğŸ“¦ ë¹Œë“œ ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
    sudo apt update
    sudo apt install -y build-essential cmake git

    # ì†ŒìŠ¤ í´ë¡ 
    if [ ! -d "$LLAMA_DIR" ]; then
        echo "ğŸ“¥ llama.cpp ì†ŒìŠ¤ í´ë¡  ì¤‘..."
        mkdir -p /home/sang/dev_ws/AI_secretary_robot/external
        cd /home/sang/dev_ws/AI_secretary_robot/external
        git clone https://github.com/ggerganov/llama.cpp.git
    fi

    cd "$LLAMA_DIR"

    # CUDA ì§€ì› ë¹Œë“œ (Jetson Orin Nano)
    echo "ğŸ”¨ llama.cpp ë¹Œë“œ ì¤‘ (CUDA ì§€ì›, ì•½ 30ë¶„ ì†Œìš”)..."
    mkdir -p build
    cd build

    cmake .. \
        -DGGML_CUDA=ON \
        -DCMAKE_CUDA_ARCHITECTURES=87 \
        -DBUILD_SHARED_LIBS=ON

    cmake --build . --config Release -j$(nproc)

    echo "âœ… llama.cpp ë¹Œë“œ ì™„ë£Œ"
fi

# ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
if [ -f "$LLAMA_DIR/build/bin/llama-server" ]; then
    sudo ln -sf "$LLAMA_DIR/build/bin/llama-server" /usr/local/bin/llama-server
    echo "âœ… llama-server ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±: /usr/local/bin/llama-server"
fi

echo ""

# ============================================
# 3. Qwen2.5-1.5B GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# ============================================
echo "[3/3] Qwen2.5-1.5B GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."

MODEL_DIR="/home/sang/dev_ws/AI_secretary_robot/models/llm"
MODEL_FILE="$MODEL_DIR/qwen2.5-1.5b-instruct-q4_k_m.gguf"

mkdir -p "$MODEL_DIR"

if [ -f "$MODEL_FILE" ]; then
    echo "âœ… GGUF ëª¨ë¸ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨: $MODEL_FILE"
else
    echo "ğŸ“¥ GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 900MB, ìˆ˜ ë¶„ ì†Œìš”)..."
    cd "$MODEL_DIR"

    wget -q --show-progress \
        https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf

    echo "âœ… GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
fi

# llama-server í…ŒìŠ¤íŠ¸
echo "ğŸ§ª llama-server í…ŒìŠ¤íŠ¸..."
if [ -f "/usr/local/bin/llama-server" ] && [ -f "$MODEL_FILE" ]; then
    echo "ğŸš€ llama-server ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì¤‘..."
    llama-server \
        --model "$MODEL_FILE" \
        --port 8081 \
        -ngl 99 \
        -c 2048 \
        -t 4 \
        &> /tmp/llama_server_test.log &

    LLAMA_PID=$!

    # ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°
    echo "â³ llama-server ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
    for i in {1..20}; do
        if curl -s http://localhost:8081/health &> /dev/null; then
            echo "âœ… llama-server ì¤€ë¹„ ì™„ë£Œ"
            break
        fi
        sleep 1
    done

    # ì¶”ë¡  í…ŒìŠ¤íŠ¸
    echo "ğŸ§ª llama-server ì¶”ë¡  í…ŒìŠ¤íŠ¸..."
    RESPONSE=$(curl -s http://localhost:8081/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
            "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
            "temperature": 0.7,
            "max_tokens": 50
        }' 2>&1)

    if echo "$RESPONSE" | grep -q "choices"; then
        echo "âœ… llama-server ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ"
    else
        echo "âš ï¸  llama-server ì‘ë‹µ: $RESPONSE"
    fi

    # ì„œë²„ ì¢…ë£Œ
    kill $LLAMA_PID 2>/dev/null || true
    echo "ğŸ›‘ llama-server í…ŒìŠ¤íŠ¸ ì¢…ë£Œ"
else
    echo "âš ï¸  llama-server ë˜ëŠ” ëª¨ë¸ íŒŒì¼ ì—†ìŒ, í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ"
fi

echo ""
echo "========================================="
echo "âœ… LLM ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ!"
echo "========================================="
echo ""
echo "ì„¤ì¹˜ëœ LLM ì—”ì§„:"
echo "  1. Ollama:        $(which ollama)"
echo "  2. llama-server:  $(which llama-server 2>/dev/null || echo 'NOT FOUND')"
echo ""
echo "ëª¨ë¸ íŒŒì¼:"
echo "  - Ollama:   $(ollama list | grep qwen2.5 || echo 'NOT FOUND')"
echo "  - GGUF:     $MODEL_FILE"
echo ""
echo "ë‹¤ìŒ ë‹¨ê³„:"
echo "  # Ollama ëª¨ë“œ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)"
echo "  ros2 launch llm_cpp llm.launch.py"
echo ""
echo "  # llama.cpp ëª¨ë“œ (params.yaml ìˆ˜ì • í•„ìš”)"
echo "  # src/ai/llm_cpp/config/params.yaml ì—ì„œ llama_server_binary ê²½ë¡œ ì„¤ì •"
echo ""
